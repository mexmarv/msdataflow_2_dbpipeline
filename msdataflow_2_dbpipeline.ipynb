{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro_section",
      "metadata": {},
      "source": [
        "# Microsoft Dataflows to Databricks Pipeline Migration\n",
        "\n",
        "This comprehensive notebook automates the migration of Microsoft Dataflows into a Databricks pipeline. It retrieves the full dataflow definition from the source API, translates each transformation step from the M language (Power Query) into equivalent PySpark code, and then automatically generates a new Databricks notebook that implements the complete pipeline.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- **Full Dataflow Extraction**: Retrieves all transformation steps (extract, filter, join, aggregate, pivot, etc.)\n",
        "- **Comprehensive Translator**: Converts a wide array of common Dataflow functions to PySpark:\n",
        "  - **Extract & Load**: Reading source files in various formats\n",
        "  - **Filtering & Conditional Logic**\n",
        "  - **Renaming & Type Conversion**\n",
        "  - **Joins, Merge, and Append**\n",
        "  - **Aggregation, Grouping and Sorting**\n",
        "  - **Pivot and Unpivot operations**\n",
        "  - **Date/Time transformations**\n",
        "  - **Text Manipulations and Replacements**\n",
        "  - **Column Splitting and Custom Scripts**\n",
        "- **Delta Lake Integration**: Optionally materializes intermediate results as Delta tables\n",
        "- **Automated Notebook Generation**: Builds a complete pipeline notebook with detailed markdown documentation for each step\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. **Databricks Environment** with PySpark configured\n",
        "2. **API Access** to your Microsoft Dataflow definitions\n",
        "3. **Python Dependencies**: `msal`, `requests`, `nbformat`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "config_section",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration Parameters\n",
        "client_id = \"YOUR_CLIENT_ID\"          # Azure AD Application (client) ID\n",
        "client_secret = \"YOUR_CLIENT_SECRET\"    # Azure AD Client Secret\n",
        "tenant_id = \"YOUR_TENANT_ID\"            # Azure AD Tenant ID\n",
        "workspace_id = \"YOUR_WORKSPACE_ID\"      # Power BI / Power Platform Workspace ID\n",
        "dataflow_id = \"YOUR_DATAFLOW_ID\"        # Dataflow ID to migrate\n",
        "dataflow_name = \"SalesDataFlow\"         # Name for the generated Databricks pipeline notebook\n",
        "materialize = True                        # Set to True to enable Delta table materialization\n",
        "\n",
        "# Uncomment if running interactively to install dependencies\n",
        "%pip install msal requests nbformat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "auth_section",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Authentication Setup\n",
        "import msal\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def get_access_token():\n",
        "    \"\"\"Obtain an access token using MSAL for the Power BI / Power Platform API\"\"\"\n",
        "    app = msal.ConfidentialClientApplication(\n",
        "        client_id,\n",
        "        authority=f\"https://login.microsoftonline.com/{tenant_id}\",\n",
        "        client_credential=client_secret\n",
        "    )\n",
        "    \n",
        "    result = app.acquire_token_for_client(scopes=[\"https://analysis.windows.net/powerbi/api/.default\"])\n",
        "    if \"access_token\" not in result:\n",
        "        raise Exception(f\"Failed to get token: {result.get('error_description')}\")\n",
        "    return result[\"access_token\"]\n",
        "\n",
        "print('Authentication module loaded.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "api_section",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataflow API Functions\n",
        "def get_dataflow_info():\n",
        "    \"\"\"Retrieve the complete Dataflow definition from the API\"\"\"\n",
        "    token = get_access_token()\n",
        "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "    \n",
        "    # Adjust URL if your API endpoint is different\n",
        "    url = f\"https://api.powerbi.com/v1.0/myorg/groups/{workspace_id}/dataflows/{dataflow_id}\"\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"API call failed: {response.text}\")\n",
        "    return response.json()\n",
        "\n",
        "def get_dataflow_steps():\n",
        "    \"\"\"Extract transformation steps from the Dataflow definition.\"\"\"\n",
        "    dataflow = get_dataflow_info()\n",
        "    # Adjust this extraction logic according to the dataflow JSON schema\n",
        "    steps = dataflow.get(\"steps\", [])\n",
        "    if not steps:\n",
        "        print(\"Warning: No transformation steps found in the dataflow definition.\")\n",
        "    return steps\n",
        "\n",
        "print('Dataflow API functions are ready.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "translator_section",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive Transformation Translation Functions\n",
        "import re\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def translate_mquery_to_pyspark(step):\n",
        "    \"\"\"\n",
        "    Translate a single Dataflow transformation step (from M language) to its corresponding PySpark code.\n",
        "    Supports a wide range of operations.\n",
        "    \"\"\"\n",
        "    step_name = step.get(\"name\", \"UnnamedStep\")\n",
        "    trans_type = step.get(\"transformationType\", \"custom\").lower()\n",
        "    details = step.get(\"details\", {})\n",
        "    code = f\"# Transformation Step: {step_name}\\n\"\n",
        "    \n",
        "    if trans_type == \"extract\":\n",
        "        # Example: Extract data from CSV, JSON, or Parquet\n",
        "        source_type = details.get(\"sourceType\", \"csv\")\n",
        "        path = details.get(\"path\", \"<source_path>\")\n",
        "        options = details.get(\"options\", {})\n",
        "        opts = \", \".join([f\".option('{k}', '{v}')\" for k, v in options.items()])\n",
        "        code += f\"df = spark.read.format('{source_type}'){opts}.load('{path}')\\n\"\n",
        "\n",
        "    elif trans_type == \"filter\":\n",
        "        condition = details.get(\"condition\", \"1=1\")\n",
        "        code += f\"df = df.filter(\\\"{condition}\\\")\\n\"\n",
        "\n",
        "    elif trans_type == \"rename\":\n",
        "        old_name = details.get(\"oldName\", \"old\")\n",
        "        new_name = details.get(\"newName\", \"new\")\n",
        "        code += f\"df = df.withColumnRenamed('{old_name}', '{new_name}')\\n\"\n",
        "\n",
        "    elif trans_type == \"select\":\n",
        "        # Select only specific columns\n",
        "        cols = details.get(\"columns\", [])\n",
        "        code += f\"df = df.select({', '.join([f'\\\"'+c+'\\\"' for c in cols])})\\n\"\n",
        "\n",
        "    elif trans_type == \"distinct\":\n",
        "        code += \"df = df.distinct()\\n\"\n",
        "\n",
        "    elif trans_type == \"sort\":\n",
        "        # Order by one or more columns\n",
        "        sort_cols = details.get(\"columns\", [])\n",
        "        ascending = details.get(\"ascending\", True)\n",
        "        code += f\"df = df.orderBy({', '.join([f'\\\"'+c+'\\\"' for c in sort_cols])}, ascending={ascending})\\n\"\n",
        "\n",
        "    elif trans_type == \"aggregate\":\n",
        "        group_by = details.get(\"groupBy\", [])\n",
        "        aggregations = details.get(\"aggregations\", {})\n",
        "        code += \"from pyspark.sql import functions as F\\n\"\n",
        "        gb_cols = \", \".join([f\"F.col('{col}')\" for col in group_by])\n",
        "        agg_expr = \", \".join([f\"F.{func.lower()}(F.col('{col}')).alias('{alias}')\" for col, (func, alias) in aggregations.items()])\n",
        "        code += f\"df = df.groupBy({gb_cols}).agg({agg_expr})\\n\"\n",
        "\n",
        "    elif trans_type == \"join\":\n",
        "        # Join two tables based on a key\n",
        "        left_table = details.get(\"leftTable\", \"left\")\n",
        "        right_table = details.get(\"rightTable\", \"right\")\n",
        "        left_key = details.get(\"leftKey\", \"id\")\n",
        "        right_key = details.get(\"rightKey\", \"id\")\n",
        "        join_type = details.get(\"joinType\", \"inner\")\n",
        "        code += (\n",
        "            f\"left_df = spark.table('{left_table}')\\n\"\n",
        "            f\"right_df = spark.table('{right_table}')\\n\"\n",
        "            f\"df = left_df.join(right_df, left_df['{left_key}'] == right_df['{right_key}'], '{join_type}')\\n\"\n",
        "        )\n",
        "\n",
        "    elif trans_type == \"pivot\":\n",
        "        # Pivot transformation\n",
        "        group_by = details.get(\"groupBy\", [])\n",
        "        pivot_column = details.get(\"pivotColumn\", \"\")\n",
        "        value_column = details.get(\"valueColumn\", \"\")\n",
        "        agg_func = details.get(\"aggFunc\", \"sum\")\n",
        "        code += (\n",
        "            f\"df = df.groupBy({', '.join([f'\\\"'+c+'\\\"' for c in group_by])}).pivot('{pivot_column}').agg(F.{agg_func}(F.col('{value_column}')))\\n\"\n",
        "        )\n",
        "\n",
        "    elif trans_type == \"unpivot\":\n",
        "        # Unpivot is not directly available in PySpark; use stack() as a workaround\n",
        "        static_cols = details.get(\"staticColumns\", [])\n",
        "        pivot_cols = details.get(\"pivotColumns\", [])\n",
        "        num_pivots = len(pivot_cols)\n",
        "        expr = \", \".join([f\"'{c}', {c}\" for c in pivot_cols])\n",
        "        code += (\n",
        "            f\"df = df.select({', '.join(static_cols)}, F.expr('stack({num_pivots}, {expr}) as (pivot_column, pivot_value)'))\\n\"\n",
        "        )\n",
        "\n",
        "    elif trans_type == \"date_transform\":\n",
        "        # Example: Add or subtract days, extract date parts\n",
        "        date_col = details.get(\"dateColumn\", \"date\")\n",
        "        operation = details.get(\"operation\", \"add\")\n",
        "        days = details.get(\"days\", 0)\n",
        "        if operation == \"add\":\n",
        "            code += f\"df = df.withColumn('{date_col}_plus', F.date_add(F.col('{date_col}'), {days}))\\n\"\n",
        "        elif operation == \"subtract\":\n",
        "            code += f\"df = df.withColumn('{date_col}_minus', F.date_sub(F.col('{date_col}'), {days}))\\n\"\n",
        "        else:\n",
        "            code += \"# Unknown date operation\\n\"\n",
        "\n",
        "    elif trans_type == \"type_conversion\":\n",
        "        # Convert a column from one type to another\n",
        "        col = details.get(\"column\", \"col\")\n",
        "        target_type = details.get(\"targetType\", \"string\")\n",
        "        code += f\"df = df.withColumn('{col}', F.col('{col}').cast('{target_type}'))\\n\"\n",
        "\n",
        "    elif trans_type == \"merge\" or trans_type == \"append\":\n",
        "        # For merging (union) two datasets\n",
        "        table1 = details.get(\"table1\", \"table1\")\n",
        "        table2 = details.get(\"table2\", \"table2\")\n",
        "        code += (\n",
        "            f\"df1 = spark.table('{table1}')\\n\"\n",
        "            f\"df2 = spark.table('{table2}')\\n\"\n",
        "            f\"df = df1.unionByName(df2)\\n\"\n",
        "        )\n",
        "\n",
        "    elif trans_type == \"replace\":\n",
        "        # Replace values in a given column\n",
        "        col = details.get(\"column\", \"col\")\n",
        "        old_value = details.get(\"oldValue\", \"\")\n",
        "        new_value = details.get(\"newValue\", \"\")\n",
        "        code += f\"df = df.replace({old_value}, {new_value}, subset=['{col}'])\\n\"\n",
        "\n",
        "    elif trans_type == \"split_column\":\n",
        "        # Split a string column by a delimiter\n",
        "        col = details.get(\"column\", \"col\")\n",
        "        delimiter = details.get(\"delimiter\", \",\")\n",
        "        new_cols = details.get(\"newColumns\", [])\n",
        "        code += f\"df = df.withColumn('{col}_split', F.split(F.col('{col}'), '{delimiter}'))\\n\"\n",
        "        if new_cols:\n",
        "            for idx, new_col in enumerate(new_cols):\n",
        "                code += f\"df = df.withColumn('{new_col}', F.col('{col}_split').getItem({idx}))\\n\"\n",
        "\n",
        "    elif trans_type == \"conditional\":\n",
        "        # Conditional transformation using when/otherwise\n",
        "        col = details.get(\"column\", \"col\")\n",
        "        condition = details.get(\"condition\", \"\")\n",
        "        true_val = details.get(\"trueValue\", \"\")\n",
        "        false_val = details.get(\"falseValue\", \"\")\n",
        "        code += f\"df = df.withColumn('{col}_cond', F.when({condition}, {true_val}).otherwise({false_val}))\\n\"\n",
        "\n",
        "    elif trans_type == \"custom\":\n",
        "        # Fallback for custom logic\n",
        "        script = details.get(\"script\", \"# Custom transformation logic not provided\")\n",
        "        code += script + \"\\n\"\n",
        "\n",
        "    else:\n",
        "        code += f\"# No translation logic implemented for transformation type: {trans_type}\\n\"\n",
        "\n",
        "    return code\n",
        "\n",
        "print('Comprehensive translator functions loaded.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "generator_section",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook Generation: Create a comprehensive Databricks Pipeline Notebook\n",
        "import nbformat as nbf\n",
        "\n",
        "def generate_pipeline_notebook():\n",
        "    \"\"\"\n",
        "    Generate a Databricks notebook that re-creates the entire Dataflow pipeline using translated PySpark code.\n",
        "    \"\"\"\n",
        "    nb = nbf.v4.new_notebook()\n",
        "    cells = []\n",
        "\n",
        "    # Introductory Markdown Cell\n",
        "    intro_md = (\n",
        "        f\"# Generated Databricks Pipeline: {dataflow_name}\\n\\n\"\n",
        "        \"This notebook was autogenerated from your Microsoft Dataflow definition. Each cell below corresponds to a transformation step translated into PySpark code.\\n\\n\"\n",
        "        \"Please review and adjust the generated code as necessary.\"\n",
        "    )\n",
        "    cells.append(nbf.v4.new_markdown_cell(intro_md))\n",
        "\n",
        "    # Retrieve the transformation steps from the dataflow definition\n",
        "    steps = get_dataflow_steps()\n",
        "    if not steps:\n",
        "        cells.append(nbf.v4.new_markdown_cell(\"**Warning:** No transformation steps found in the dataflow definition.\"))\n",
        "    \n",
        "    # Create a Markdown cell and a Code cell for each transformation step\n",
        "    for idx, step in enumerate(steps, start=1):\n",
        "        step_name = step.get(\"name\", f\"Step {idx}\")\n",
        "        desc = step.get(\"description\", \"\")\n",
        "        md_text = f\"## Transformation {idx}: {step_name}\\n{desc}\"\n",
        "        cells.append(nbf.v4.new_markdown_cell(md_text))\n",
        "        \n",
        "        code = translate_mquery_to_pyspark(step)\n",
        "        cells.append(nbf.v4.new_code_cell(code))\n",
        "\n",
        "    # Append Delta Lake materialization cell if enabled\n",
        "    if materialize:\n",
        "        mat_md = \"### Delta Table Materialization Enabled\\nThe following cell creates a Delta table based on the pipeline output.\"\n",
        "        cells.append(nbf.v4.new_markdown_cell(mat_md))\n",
        "        mat_code = (\n",
        "            \"# Sample Delta table creation\\n\"\n",
        "            \"df.write.format('delta').mode('overwrite').saveAsTable('semantic.materialized_output')\\n\"\n",
        "            \"print('Delta table created.')\\n\"\n",
        "        )\n",
        "        cells.append(nbf.v4.new_code_cell(mat_code))\n",
        "\n",
        "    nb['cells'] = cells\n",
        "\n",
        "    # Build the output filename and save the notebook\n",
        "    output_filename = f\"pipeline_{dataflow_name.replace(' ', '_').lower()}_databricks.ipynb\"\n",
        "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "        nbf.write(nb, f)\n",
        "\n",
        "    print(f\"Generated Databricks pipeline notebook: {output_filename}\")\n",
        "    return output_filename\n",
        "\n",
        "print('Pipeline notebook generation functions ready.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "execution_section",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute Pipeline Notebook Generation\n",
        "generated_notebook = generate_pipeline_notebook()\n",
        "print(f\"Migration complete. The generated Databricks pipeline notebook is saved as: {generated_notebook}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
